{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mechanicalsoup\n",
    "import bs4, re, lxml, unidecode,codecs\n",
    "import base64,glob,os\n",
    "from appscript import *\n",
    "import cellbell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = mechanicalsoup.Browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download='L1VzZXJzL2dpbGxlcy9Eb3dubG9hZHMv'\n",
    "vrac='L1ZvbHVtZXMvZ2lsbGVzL1RyYW5zZmVydC9WcmFjMi8='\n",
    "linkFilter=[\"c2l0ZXJpcA==\",\"bGVzYmlhbg==\",\"MTlbNzg5XVxk\",\"c2hlbWFsZQ==\",\"dHJhbnM=\",'W3NTXXF1aXJ0']\n",
    "urlBase='aHR0cDovL2Z1bGx4eHhtb3ZpZXMubmV0L3BhZ2UvJWQv'\n",
    "numLettres=\"ABCDEFGHIJKLMNOP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numPage=1\n",
    "nbPages=4\n",
    "extension=\"\"\n",
    "safariLoad=False\n",
    "boolPrint=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en(s):\n",
    "    return base64.b64encode(s)\n",
    "def de(s):\n",
    "    return base64.b64decode(s)\n",
    "de(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getSoup(url):\n",
    "    page = browser.get(url)\n",
    "    soup = bs4.BeautifulSoup(page.content,\"lxml\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubLinks(soup,tag=\"article\"):\n",
    "    links=[]\n",
    "    for article in soup.find_all(tag):\n",
    "        href=article.find('a')[\"href\"]\n",
    "        if linkFilter:\n",
    "            lF=[de(l) for l in linkFilter]\n",
    "            match=\"(\"+\"|\".join(lF)+\")\"\n",
    "            m=re.search(match,href)\n",
    "            if not m:\n",
    "                links.append(article.find('a')[\"href\"])\n",
    "        else:\n",
    "            links.append(article.find('a')[\"href\"])\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterASoup(soup,chaine,splitter,link):\n",
    "    for a in soup.find_all('a'):\n",
    "        href=a.get(\"href\")\n",
    "        if href and chaine in href:\n",
    "            link=href.split(splitter)[-1]\n",
    "    return link\n",
    "\n",
    "def getTwitterSecurelyLink(url):\n",
    "    link=url\n",
    "    soup=getSoup(url)\n",
    "    link=filterASoup(soup,\"url=https://openload.co\",\"url=\",link)\n",
    "    if link==url:\n",
    "        link=filterASoup(soup,\"u=https://openload.co\",\"u=\",link)\n",
    "    if link==url:\n",
    "        print \"no share link\"\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSecurelyLink(url):\n",
    "    link=url\n",
    "    for meta in getSoup(url).find_all(\"meta\"):\n",
    "        content=meta.get(\"content\")\n",
    "        if content and \"://openload.co\" in content:\n",
    "            link=content\n",
    "    if link==url:\n",
    "        print \"no content link\",link\n",
    "        link=getTwitterSecurelyLink(url)\n",
    "    elif \"securely.link\" in link:\n",
    "        print \"recursive securely\",link\n",
    "        link=getSecurelyLink(link)\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpenloadLinks(soup):\n",
    "    links=[]\n",
    "    for article in soup.find_all(\"div\",class_=\"entry-content\"):\n",
    "        aSoup=article.find_all('a')\n",
    "        for a in aSoup:\n",
    "            link=a[\"href\"]\n",
    "            if u\"openload.co\" in link:\n",
    "                links.append(link)\n",
    "            elif u\"openload.co\" in a.text.lower() and u\"securely.link\" in link:\n",
    "                links.append(getSecurelyLink(link))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNextPage(soup):\n",
    "    try:\n",
    "        nextPage=soup.find(\"a\", class_=\"next page-numbers\")[\"href\"]\n",
    "    #    print nextPage.rsplit(\"/\")[-1]\n",
    "        return getSoup(nextPage)\n",
    "    except:\n",
    "        try:\n",
    "            nextPage=soup.find('div', class_=\"loadMoreInfinite\").find('a')[\"href\"]\n",
    "            print nextPage.rsplit(\"/\")[-1]\n",
    "            return getSoup(nextPage)\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pageLoop(urls,safariLoad=True,boolPrint=False,missing=[]):\n",
    "    olURLs={}\n",
    "    for url in urls:\n",
    "        urlSoup=getSoup(url)\n",
    "        for nLink,link in enumerate(getOpenloadLinks(urlSoup)):\n",
    "            if u\"openload.co\" in link:\n",
    "                sLink=link.rsplit(\"/\")[-1]\n",
    "            else:\n",
    "                sLink=link\n",
    "            if sLink[-3:]==\"%20\":\n",
    "                sLink=sLink[:-3]\n",
    "#            print sLink\n",
    "            if not sLink[-4:]==\".mp4\":\n",
    "                sLink+=\".mp4\"\n",
    "            boolMissing=sLink in missing\n",
    "            if (boolPrint or safariLoad) and not boolMissing:\n",
    "                print urlSoup.title.text\n",
    "                print sLink\n",
    "                print link\n",
    "            fileNames[sLink]=urlSoup.title.text+\"-%d\"%nLink\n",
    "            olURLs[sLink]=link\n",
    "            if safariLoad and not boolMissing:\n",
    "                ok=raw_input(\"Press Enter to continue...\")\n",
    "                if ok.lower()!=\"n\":\n",
    "                    safari.windows.first.make(new=k.tab,with_properties={k.URL:link})\n",
    "                else:\n",
    "                    break\n",
    "            elif boolMissing:\n",
    "                safari.windows.first.make(new=k.tab,with_properties={k.URL:link})\n",
    "    return olURLs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# accueil.soup is a BeautifulSoup object http://www.crummy.com/software/BeautifulSoup/bs4/doc/#beautifulsoup \n",
    "# we grab the search form\n",
    "formulaire = accueil.soup.select(\"form\")[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# fill out the search in the right box\n",
    "formulaire.select(\"#ctl00_cphMainContent_m_ctrlSearchEngine_m_ctrlRechercheForm_m_txtSearch\")[0][\"value\"]=\"tarte tatin\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# submit form\n",
    "recherche = browser.submit(formulaire, accueil.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dictinvert(d):\n",
    "    inv = {}\n",
    "    for k, v in d.iteritems():\n",
    "        keys = inv.setdefault(v, [])\n",
    "        keys.append(k)\n",
    "    return inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeName(name):\n",
    "    result=\"\".join([l.capitalize() for l in name.rsplit(\"-\",1)[0].rsplit(\"(\",1)[0].split(\" \") if l!=\"\"])\n",
    "    result=result.replace(\"-\",\"\").replace(\"!\",\"\").replace(\"'\",\"\").replace(\":\",\"\").replace(\"/\",\"\")\n",
    "    return unidecode.unidecode(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suffixName(element):\n",
    "    element=element.split(\"/\")[-1]\n",
    "    nameKey,suffix=fileNames[element].rsplit(\"-\",1)\n",
    "    suffix=int(suffix)\n",
    "    name=normalizeName(fileNames[element])\n",
    "    if len(nameFiles[nameKey])>1:\n",
    "        name+=numLettres[suffix]    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripName(name):\n",
    "    return unidecode.unidecode(name.rsplit(\"-\",1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRepFiles(repName=de(download)):\n",
    "    return [l.rsplit(\"/\")[-1] for l in glob.glob(repName+u\"*.mp4\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDelFiles():\n",
    "    with codecs.open(de(vrac)+\"deleted.txt\") as input:\n",
    "        result=[l.strip() for l in input.readlines()]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPages():\n",
    "    olURLs={}\n",
    "    localPage=page\n",
    "    print linkPage.rsplit(\"/\")[-1]\n",
    "    for i in range(nbPages):\n",
    "        print \"page %02d\"%i\n",
    "        urls=getSubLinks(localPage)\n",
    "        olURLs.update(pageLoop(urls,safariLoad=safariLoad,boolPrint=boolPrint,missing=missing))\n",
    "        localPage = getNextPage(localPage)\n",
    "        if not localPage:\n",
    "            break\n",
    "    return olURLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSelectNums(selectNums):\n",
    "    missingNums=[]\n",
    "    for num in selectNums:\n",
    "        key=sorted(nameFiles.keys())[num]\n",
    "        print key\n",
    "        missingNums.extend(nameFiles[key])\n",
    "    print missingNums\n",
    "    missingURLs=[olURLs[name] for name in missingNums if name in olURLs]\n",
    "    return missingURLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSelection(selection):\n",
    "    for link in selection:\n",
    "        safari.windows.first.make(new=k.tab,with_properties={k.URL:link})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safari = app(\"Safari\")\n",
    "repName=de(download)\n",
    "fileNames={}\n",
    "#nameFiles={}\n",
    "try:\n",
    "    missing\n",
    "except NameError:\n",
    "    missing = []\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numPage=1\n",
    "nbPages=4\n",
    "extension=u\"\"\n",
    "linkPage=de(urlBase)%numPage\n",
    "page=getSoup(linkPage+extension)\n",
    "urls=getSubLinks(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllFiles():\n",
    "    return getRepFiles(de(vrac))+getRepFiles()+getDelFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print getSoup(urls[0]).find(\"title\").text\n",
    "olURLs=getPages()\n",
    "\n",
    "nameFiles=dictinvert({f:stripName(fileNames[f]) for f in fileNames})\n",
    "repFiles=getRepFiles(de(vrac))+getRepFiles()+getDelFiles()\n",
    "for num,name in enumerate(sorted(nameFiles.keys())):\n",
    "    sub=normalizeName(name)\n",
    "    if filter(lambda x: sub in x, repFiles):\n",
    "        print \"\\t%02d+\"%num,name\n",
    "    else:\n",
    "        print \"%02d \"%num,name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strSelect=raw_input(\"Selected numbers\\n\")\n",
    "if strSelect:\n",
    "    selectNums=strSelect.split()\n",
    "    selectNums=[int(n) for n in selectNums]\n",
    "    print selectNums\n",
    "    missing=getSelectNums(selectNums)\n",
    "print len(missing)\n",
    "strGetPages=raw_input(\"Get pages\\n\")\n",
    "if strGetPages==\"y\":\n",
    "    getSelection(missing)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repFiles=getRepFiles()\n",
    "for element in sorted(repFiles):\n",
    "    if element in fileNames:\n",
    "        name=suffixName(element)\n",
    "        print element, name\n",
    "        os.rename(repName+element,repName+name+\".mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateMissing():\n",
    "    allFiles=getAllFiles()\n",
    "    downFiles=getRepFiles()\n",
    "    newMissing=[]\n",
    "    for element in missing:\n",
    "        if not element[-4:]==\".mp4\":\n",
    "            element+=\".mp4\"\n",
    "        elementFile=element.split(\"/\")[-1]\n",
    "        elementName=suffixName(element)\n",
    "        if not elementName+\".mp4\" in allFiles and not elementFile in downFiles:\n",
    "            print \"-\",elementName\n",
    "            newMissing.append(element)\n",
    "        else:\n",
    "            print \"+\", elementName\n",
    "    print len(newMissing)\n",
    "    return newMissing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updateMissing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSelection(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olURLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printLang(chaine):\n",
    "    print detect(chaine), chaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listeExemples=[\n",
    "    u\"Le Concours Eurovision de la chanson est un événement annuel organisé par l'Union européenne de radio-télévision, l'UER. Il réunit les membres de l'Union dans le cadre d'une compétition musicale, diffusée en direct et en simultané par tous les diffuseurs participants. Il est retransmis à la télévision (par câble et satellite), la radio et sur Internet.\",\n",
    "    u\"Oxygen is a chemical element with symbol O and atomic number 8. A member of the chalcogen group on the periodic table, it is a highly reactive nonmetal and oxidizing agent that forms oxides with most elements as well as other compounds. By mass, oxygen is the most abundant element in the Earth's crust and the third-most abundant in the universe, after hydrogen and helium.\",\n",
    "    u\"Сам зачетак рата огледа се у незадовољству многих племића и младих самураја према политици коју је шогунат водио према странцима. Јапан је дуго био затворен за странце услед изолационе „сакоку“ политике које је донета пре 220 година. Када је адмирал Метју Пери дошао у Јапан и наредио поновно отварање граница под претњом силе, шогунат је на претњу одговорио потписивањем разних мировних споразума често и на штету самог Јапана. Имајући у виду скорашњу смрт владајућег шогуна Токугаве Ијесаде (шогун је поред цара био де факто владар Јапана) и технолошку предност развијенијег Запада, власт није имала превише избора којим би могла прибећи.\",\n",
    "    u\"Trans-Antartika Espedizio Inperiala (1914-1917), Endurance Espedizioa izenaz ere ezaguna, Antartikako Heroi Aroko Esplorazioaren azken espedizio garrantzitsua izan zen. Sir Ernest Shackleton buruzagi eta diseinatzaile izan zen eta Antartika alderik alde zeharkatzeko helburua zuen. 1911n Roald Amundsen norvegiarra Hego Polora iritsi ondoren, Shackeltonek zioenez, Antartikarako bidaien helburu nagusi bakarra kontinentea zeharkatzea zen. Bidaia horren distantzia 2.900 kilometrokoa zen, eta ibilbidearen erdia, hots, Weddell itsasoa eta Hego Polo artekoa, artean esploratzeke zegoen. Ezin izan zuten helburua bete, baina heroi epika eta biziraupen istorio modura gogoratzen da.\"\n",
    "]\n",
    "for element in listeExemples:\n",
    "    for phrase in element.split(\".\"):\n",
    "        if phrase!=\"\": printLang(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de(urlBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fNames={}\n",
    "fNamesOld=fileNames\n",
    "for k in fileNames:\n",
    "    if \"%\" in k:\n",
    "        newK=k.split(\"%\")[0]\n",
    "        if not newK.endswith(\".mp4\"):\n",
    "            newK+=\".mp4\"\n",
    "        fNames[newK]=fileNames[k]\n",
    "    else:\n",
    "        fNames[k]=fileNames[k]\n",
    "fNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "olURLs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
